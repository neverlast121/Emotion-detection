{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'images_folder/download (4).jfif'\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# cv2.imshow('Image', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  \n",
    "plt.show()\n",
    "\n",
    "start = time.monotonic()\n",
    "obj = DeepFace.analyze(img_path=image_path, actions=['emotion'],  detector_backend = 'ssd')\n",
    "results = obj[\"dominant_emotion\"]\n",
    "end = time.monotonic()\n",
    "\n",
    "print('Inference time:', end - start)\n",
    "\n",
    "print('The detected emotion:', results)\n",
    "\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code will perform the detection output on most common in 5 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from deepface import DeepFace\n",
    "from collections import Counter\n",
    "video = 'C:/Users/never/Desktop/activity-recognition/video_1.mp4'\n",
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define a counter to store emotion results\n",
    "emotion_counter = Counter()\n",
    "\n",
    "# Define the number of frames to analyze for each result\n",
    "num_frames = 5\n",
    "\n",
    "while True:\n",
    "    for _ in range(num_frames):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        emotion = []\n",
    "        # Our operations on the frame come here\n",
    "        start = time.monotonic()\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'], \n",
    "                                  enforce_detection=False,  \n",
    "                                  # detector_backend = 'retinaface',\n",
    "                                  # detector_backend = 'mtcnn', \n",
    "                                  # detector_backend ='opencv',\n",
    "                                  detector_backend = 'ssd'\n",
    "                                )\n",
    "        end = time.monotonic()\n",
    "        # Update the counter with the dominant emotion\n",
    "        emotion_counter.update([result[\"dominant_emotion\"]])\n",
    "       \n",
    "    # Display the most common emotion over the last num_frames frames\n",
    "    most_common_emotion, _ = emotion_counter.most_common(1)[0]\n",
    "    print(most_common_emotion)\n",
    "    print('inference time:', end - start )\n",
    "    # Draw the emotion text on the frame\n",
    "    cv2.putText(frame, f'Emotion: {most_common_emotion}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Webcam Emotion Recognition', frame)\n",
    "\n",
    "    # Reset the counter for the next batch of frames\n",
    "    emotion_counter.clear()\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code will perform the dectection on every frame of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame):\n",
    "    _, buffer = cv2.imencode('.jpg', frame)\n",
    "    image = buffer.tobytes()\n",
    "    clear_output(wait=True)\n",
    "    display(Image(data=image))\n",
    "\n",
    "video_path = ('C:/Users/never/Desktop/web-site-development/backend/database/videos/recorded_video.webm')\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    obj = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False, detector_backend='ssd')\n",
    "\n",
    "    results = obj[\"dominant_emotion\"]\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, f'Emotion: {results}', (10, 40), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    display_frame(frame)\n",
    "\n",
    "    print(obj)\n",
    "    print('The detected emotion:', results)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
